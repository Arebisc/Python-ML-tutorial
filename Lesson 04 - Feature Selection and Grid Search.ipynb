{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 04 - Feature selection and Grid search\n",
    "\n",
    "In this lesson, you will use a simple technique to tune hyperparameters of your models called GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:06.750006Z",
     "start_time": "2019-12-17T10:05:04.593724Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and basic info\n",
    "\n",
    "Let's load the same dataset as in Lesson 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:07.062115Z",
     "start_time": "2019-12-17T10:05:06.752341Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bugs = pd.read_csv('./data/bugs_train.csv', parse_dates=['Opened', 'Changed'], index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:07.210684Z",
     "start_time": "2019-12-17T10:05:07.077497Z"
    }
   },
   "outputs": [],
   "source": [
    "bugs.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The classification task (the problem to solve)\n",
    "\n",
    "Our task remains the same for this lesson - we would be to predict what will be the resolution of the defect report (y) based on the description of a defect (X). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation (features)\n",
    "\n",
    "Let's quickly replicate processing of the Component and Severity features, as well as converting the decision class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:21.079338Z",
     "start_time": "2019-12-17T10:05:07.218883Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will make a copy of the main data\n",
    "bugs_small = bugs[[\"Assignee\", \"Component\", \"Severity\", \"Status\", \"Priority\", \"Opened\", \"Changed\", \"Summary\", \"Resolution\"]]\n",
    "\n",
    "# Component\n",
    "bugs_small = pd.get_dummies(bugs_small, columns=['Component'], prefix=\"Component\")\n",
    "\n",
    "# Severity\n",
    "bugs_small['Severity'] = bugs_small['Severity'].map(\n",
    "    {'enhancement':0, 'trivial':1, 'minor':2, 'normal':3, 'major':4, 'critical':5, 'blocker':6})\n",
    "\n",
    "# Status\n",
    "bugs_small['Status'] = bugs_small['Status'].map(\n",
    "    {'VERIFIED':0, 'RESOLVED':1, 'CLOSED':2})\n",
    "\n",
    "# Priority\n",
    "bugs_small['Priority'] = bugs_small['Priority'].map(\n",
    "    {'P1':1, 'P2':2, 'P3':3, 'P4':4, 'P5':5})\n",
    "\n",
    "\n",
    "y = bugs_small['Resolution']\n",
    "X = bugs_small.drop(['Resolution'], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "# Days\n",
    "X['Days'] = X.apply(lambda x: (x.Changed - x.Opened).days, axis=1)\n",
    "X.drop([\"Changed\", \"Opened\"], inplace=True, axis=1)\n",
    "\n",
    "# Summary as BoW\n",
    "X['Summary'] = X['Summary'].fillna('')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create CountVectorizer class; we take only 30 most frequently appearing features\n",
    "count_vect = CountVectorizer(max_features=30, stop_words=\"english\")\n",
    "\n",
    "# CountVectorizer fit method extracts vocabulary while transform performs the transformation. There is also\n",
    "# the method fit_transform that does both.\n",
    "bag_of_words = count_vect.fit_transform(list(X['Summary'])).todense()\n",
    "\n",
    "# We create a list of names of columns \n",
    "colnames = [\"Summary_\"+x for x in sorted(count_vect.vocabulary_.keys())]\n",
    "\n",
    "# Finally, we create a dataframe with bag of words features\n",
    "summary_bow = pd.DataFrame(bag_of_words, columns=colnames)\n",
    "X = pd.concat([X.reset_index(drop=True), summary_bow], axis=1)\n",
    "X.drop([\"Summary\"], inplace=True, axis=1)\n",
    "\n",
    "# Assignee\n",
    "inbox = [1 if x.endswith('-inbox') else 0 for x in bugs_small['Assignee']]\n",
    "X.insert(loc=0, column='Assignee_Inbox', value=pd.Series(inbox))\n",
    "    \n",
    "eclipse = [1 if x.endswith('eclipse') else 0 for x in bugs_small['Assignee']]\n",
    "X.insert(loc=0, column='Assignee_Eclipse', value=pd.Series(eclipse))\n",
    "X.drop([\"Assignee\"], inplace=True, axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create an instance of the class\n",
    "y_encoder = LabelEncoder()\n",
    "\n",
    "# fit the converter to the data\n",
    "y_encoder.fit(y)\n",
    "\n",
    "# let's see the mapping\n",
    "for y_label in y.unique():\n",
    "    print(y_label, y_encoder.transform([y_label]))\n",
    "\n",
    "# convert y to numbers\n",
    "y = y_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:21.274613Z",
     "start_time": "2019-12-17T10:05:21.092881Z"
    }
   },
   "outputs": [],
   "source": [
    "X.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature normalization\n",
    "\n",
    "It is very likely that your features will use different scales. It may have a big impact on some of the machine learning algorithms. Also, it is often required to preserve a certain type of probability distribution (e.g. normal). \n",
    "\n",
    "In the following steps, we will learn two basic transformations -> min-max normalization, and standardization. You can find more scalers in the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:21.299104Z",
     "start_time": "2019-12-17T10:05:21.288512Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max scaler\n",
    "\n",
    "It changes the range of the variable by normalizing all the original values with their range. As a result, the scale range is between 0 and 1 and the original distribution shape is preserved. This method is very useful for preparing input for the neural networks or algorithms that calculate distance (e.g., K-NN).\n",
    "\n",
    "Please note that the scaler can have a different output range than 0-1, but this is how we usually use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:21.380695Z",
     "start_time": "2019-12-17T10:05:21.304431Z"
    }
   },
   "outputs": [],
   "source": [
    "# We create a scaler\n",
    "minmax_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# We need to provide it a sample data (here we will use the whole dataset)\n",
    "minmax_scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:21.432028Z",
     "start_time": "2019-12-17T10:05:21.387054Z"
    }
   },
   "outputs": [],
   "source": [
    "# We use it like this\n",
    "minmax_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:21.876665Z",
     "start_time": "2019-12-17T10:05:21.462936Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can pack it back to a DataFrame\n",
    "X_minmax = pd.DataFrame(minmax_scaler.transform(X), columns=X.columns)\n",
    "X_minmax.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare distributions of three types of features (scale-wise) before and after the transformation: Days, Severity, and Assignee_Eclipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:23.325804Z",
     "start_time": "2019-12-17T10:05:21.886057Z"
    }
   },
   "outputs": [],
   "source": [
    "X.Days.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"Before\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "X_minmax.Days.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"After\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:24.637624Z",
     "start_time": "2019-12-17T10:05:23.328725Z"
    }
   },
   "outputs": [],
   "source": [
    "X.Severity.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"Before\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "X_minmax.Severity.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"After\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:26.563805Z",
     "start_time": "2019-12-17T10:05:24.642247Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.Assignee_Eclipse.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"Before\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "X_minmax.Assignee_Eclipse.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"After\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard scaler\n",
    "\n",
    "This one standardizes features by substracting the mean value and then scaling to unit variance (mean = 0, SD = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:26.677415Z",
     "start_time": "2019-12-17T10:05:26.570124Z"
    }
   },
   "outputs": [],
   "source": [
    "# We create a scaler\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# We need to provide it a sample data (here we will use the whole dataset)\n",
    "std_scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:26.768509Z",
     "start_time": "2019-12-17T10:05:26.685582Z"
    }
   },
   "outputs": [],
   "source": [
    "# We use it like this\n",
    "std_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:26.984627Z",
     "start_time": "2019-12-17T10:05:26.775848Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can pack it back to a DataFrame\n",
    "X_std = pd.DataFrame(std_scaler.transform(X), columns=X.columns)\n",
    "X_std.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare distributions of three types of features (scale-wise) before and after the transformation: Days, Severity, and Assignee_Eclipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:28.246348Z",
     "start_time": "2019-12-17T10:05:26.992593Z"
    }
   },
   "outputs": [],
   "source": [
    "X.Days.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"Before\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "X_std.Days.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"After\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:29.324569Z",
     "start_time": "2019-12-17T10:05:28.253067Z"
    }
   },
   "outputs": [],
   "source": [
    "X.Severity.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"Before\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "X_std.Severity.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"After\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:30.559597Z",
     "start_time": "2019-12-17T10:05:29.327934Z"
    }
   },
   "outputs": [],
   "source": [
    "X.Assignee_Eclipse.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"Before\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "X_std.Assignee_Eclipse.hist(bins=100, alpha=0.8)\n",
    "plt.title(\"After\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature selection\n",
    "\n",
    "There are multiple algorithms for features selection available. The main distinction is between so-called filters, wrappers, and embedded methods. The former tries to evaluate the importance of features by analyzing its relationship between other features or response variable. The second use a given prediction algorithm to construct multiple models using different features and by evaluating their performance they judge which features are potentially the most valuable ones. Finally, the latter are built-in into some of the algorithms (e.g., some ensambles like random forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:30.720038Z",
     "start_time": "2019-12-17T10:05:30.563740Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_selection as feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low variance\n",
    "\n",
    "It is a very simple filter which goal is to remove features which variance is smaller than a given threshold. It is the one that it is good to start from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:30.892588Z",
     "start_time": "2019-12-17T10:05:30.724026Z"
    }
   },
   "outputs": [],
   "source": [
    "# We create a filter\n",
    "var_filter = feature_selection.VarianceThreshold(threshold=0.0) # remove all features with zero variance\n",
    "\n",
    "# We need to provide it a sample data (here we will use the whole dataset)\n",
    "var_filter.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:30.918804Z",
     "start_time": "2019-12-17T10:05:30.896062Z"
    }
   },
   "outputs": [],
   "source": [
    "# We use it like this\n",
    "var_filter.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:31.152044Z",
     "start_time": "2019-12-17T10:05:30.924981Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can pack it back to a DataFrame\n",
    "X_zero_var = pd.DataFrame(var_filter.transform(X), columns=X.columns)\n",
    "X_zero_var.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T08:17:51.087735Z",
     "start_time": "2019-12-17T08:17:51.083302Z"
    },
    "collapsed": true
   },
   "source": [
    "### Filter - Correlation between features\n",
    "\n",
    "One of the basic filter methos is to remove some of the higly correlated features.\n",
    "\n",
    "Here a short remark. Always use the appropriate correlation coefficient for the type of data! For simiplicity, we will use Spearman correlation coefficient below, but it is not a good idea to use it for the nominal data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:43.660638Z",
     "start_time": "2019-12-17T10:05:31.180960Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,15))\n",
    "corrs = X.corr(method=\"spearman\") #pearson, spearman or kendall\n",
    "#sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "sns.heatmap(corrs, vmin=-1.0, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, cmap='coolwarm')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the correlations - what can you learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter - Relationship with the output variable\n",
    "\n",
    "Sklearn offers a few classes for filtering features based on their relationship with the output variables (see https://scikit-learn.org/stable/modules/feature_selection.html). Here, we wil go through a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:43.751487Z",
     "start_time": "2019-12-17T10:05:43.663948Z"
    }
   },
   "outputs": [],
   "source": [
    "# We create a filter\n",
    "kbest_filter = feature_selection.SelectKBest(feature_selection.chi2, k=10) \n",
    "\n",
    "# We need to provide it a sample data (here we will use the whole dataset)\n",
    "kbest_filter.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:43.765627Z",
     "start_time": "2019-12-17T10:05:43.753824Z"
    }
   },
   "outputs": [],
   "source": [
    "# We use it like this\n",
    "kbest_filter.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:43.824932Z",
     "start_time": "2019-12-17T10:05:43.767991Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can pack it back to a DataFrame\n",
    "cols = kbest_filter.get_support(indices=True)\n",
    "X_kbest = pd.DataFrame(kbest_filter.transform(X), columns=X.columns[cols])\n",
    "X_kbest.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper - RFE\n",
    "\n",
    "Feature ranking with recursive feature elimination. You can also look at the RFECV which does a similar thing but uses cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:46.297572Z",
     "start_time": "2019-12-17T10:05:43.829963Z"
    }
   },
   "outputs": [],
   "source": [
    "# First we need the prediction model for wrapping. Let it be a simple decision tree\n",
    "from sklearn import tree\n",
    "predictor = tree.DecisionTreeClassifier()\n",
    "\n",
    "# We create a filter\n",
    "rfe_filter = feature_selection.RFE(predictor, n_features_to_select=10, step=5) \n",
    "\n",
    "# We need to provide it a sample data (here we will use the whole dataset)\n",
    "rfe_filter.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:46.318798Z",
     "start_time": "2019-12-17T10:05:46.302218Z"
    }
   },
   "outputs": [],
   "source": [
    "# We use it like this\n",
    "rfe_filter.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:46.457451Z",
     "start_time": "2019-12-17T10:05:46.323869Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can pack it back to a DataFrame\n",
    "cols = rfe_filter.get_support(indices=True)\n",
    "X_rfe = pd.DataFrame(rfe_filter.transform(X), columns=X.columns[cols])\n",
    "X_rfe.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "You can combine multiple scalers, selectors, classifiers, ... into a pipleline. It will have the same API as a prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:46.523259Z",
     "start_time": "2019-12-17T10:05:46.460820Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest\n",
    "\n",
    "pipeline = Pipeline([\n",
    "  ('min_max_scaler', preprocessing.MinMaxScaler()),\n",
    "  ('low_variance_filter', VarianceThreshold(threshold=0.0)),\n",
    "  ('kbest_chi2_filter', SelectKBest(feature_selection.chi2, k=30)),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:46.700124Z",
     "start_time": "2019-12-17T10:05:46.528960Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:50.466648Z",
     "start_time": "2019-12-17T10:05:46.705481Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_pred = cross_val_predict(pipeline, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:50.583669Z",
     "start_time": "2019-12-17T10:05:50.469291Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "acc = accuracy_score(y, y_pred)\n",
    "prec = precision_score(y, y_pred, average='macro')\n",
    "rec = recall_score(y, y_pred, average='macro')\n",
    "f1 = f1_score(y, y_pred, average='macro')\n",
    "\n",
    "\"Accuracy = {:.3f}, Precision = {:.3f}, Recall = {:.3f}, F1-score = {:.3f}\".format(acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:51.807062Z",
     "start_time": "2019-12-17T10:05:50.592063Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_confusion_matrix(cnf_matrix, classes=y_encoder.classes_,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=y_encoder.classes_, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "\n",
    "Feature engineering is not the only way to improve predictions. Sometimes, it is also about choosing the best values of hyperparameters of the ML algorithm we are using. A brute-force way of doing this is called Grid Search. Yes, it is that simple - just try different combinations of parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:51.814843Z",
     "start_time": "2019-12-17T10:05:51.811010Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to tune a random forest classifier. Let's try with two hyperparameters:\n",
    "* n_estimators \n",
    "* max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:05:51.825142Z",
     "start_time": "2019-12-17T10:05:51.818148Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\" : (10, 30, 50),\n",
    "    \"max_depth\" : (2, 3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:06:06.804866Z",
     "start_time": "2019-12-17T10:05:51.832083Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "search = GridSearchCV(estimator=RandomForestClassifier(), \n",
    "                      param_grid=params, \n",
    "                      cv=5, \n",
    "                      scoring=make_scorer(accuracy_score))\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:06:06.820863Z",
     "start_time": "2019-12-17T10:06:06.807876Z"
    }
   },
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:06:06.871729Z",
     "start_time": "2019-12-17T10:06:06.824478Z"
    }
   },
   "outputs": [],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tune the parameters of models in the pipeline (we use double underscore to refer to members of the classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:06:06.938292Z",
     "start_time": "2019-12-17T10:06:06.882312Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "  ('min_max_scaler', preprocessing.MinMaxScaler()),\n",
    "  ('low_variance_filter', VarianceThreshold(threshold=0.0)),\n",
    "  ('kbest_chi2_filter', SelectKBest(feature_selection.chi2, k=30)),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    \"kbest_chi2_filter__k\" : (10, 30),\n",
    "    \"classification__n_estimators\" : (10, 30),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:06:33.098373Z",
     "start_time": "2019-12-17T10:06:06.945858Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "search = GridSearchCV(estimator=pipeline, \n",
    "                      param_grid=params, \n",
    "                      cv=5, \n",
    "                      scoring=make_scorer(accuracy_score))\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T10:06:33.113572Z",
     "start_time": "2019-12-17T10:06:33.102823Z"
    }
   },
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Use different scalers, feature selectors, and parameters to find the best model to predict bugs resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
