{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 03 - Classification, text, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and basic info\n",
    "\n",
    "Let's load the same dataset as in Lesson 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bugs = pd.read_csv('./data/bugs_train.csv', parse_dates=['Opened', 'Changed'], index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bugs.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The classification task (the problem to solve)\n",
    "\n",
    "Our task remains the same for this lesson - we would be to predict what will be the resolution of the defect report (y) based on the description of a defect (X). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation (features)\n",
    "\n",
    "Let's quickly replicate processing of the Component and Severity features, as well as converting the decision class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will make a copy of the main data\n",
    "bugs_small = bugs[[\"Component\", \"Severity\", \"Status\", \"Priority\", \"Opened\", \"Changed\", \"Summary\", \"Resolution\"]]\n",
    "\n",
    "# Component\n",
    "bugs_small = pd.get_dummies(bugs_small, columns=['Component'], prefix=\"Component\")\n",
    "\n",
    "# Severity\n",
    "bugs_small['Severity'] = bugs_small['Severity'].map(\n",
    "    {'enhancement':0, 'trivial':1, 'minor':2, 'normal':3, 'major':4, 'critical':5, 'blocker':6})\n",
    "\n",
    "# Status\n",
    "bugs_small['Status'] = bugs_small['Status'].map(\n",
    "    {'VERIFIED':0, 'RESOLVED':1, 'CLOSED':2})\n",
    "\n",
    "# Priority\n",
    "bugs_small['Priority'] = bugs_small['Priority'].map(\n",
    "    {'P1':1, 'P2':2, 'P3':3, 'P4':4, 'P5':5})\n",
    "\n",
    "y = bugs_small['Resolution']\n",
    "X = bugs_small.drop(['Resolution'], axis=1, inplace=False)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create an instance of the class\n",
    "y_encoder = LabelEncoder()\n",
    "\n",
    "# fit the converter to the data\n",
    "y_encoder.fit(y)\n",
    "\n",
    "# let's see the mapping\n",
    "for y_label in y.unique():\n",
    "    print(y_label, y_encoder.transform([y_label]))\n",
    "\n",
    "# convert y to numbers\n",
    "y = y_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates - days being processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on features we could create from two dates Opened and Changed. In the form they are right now, they are not usable as features. We could convert each of them to set of features, like year, month, day, etc. We can also think about new features that somehow combain both dates. Let's create a feature that will be the number of days the defect is being repaired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using lambda function\n",
    "X['Days'] = X.apply(lambda x: (x.Changed - x.Opened).days, axis=1)\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using iteration\n",
    "days_processed = [x.days for x in (X['Changed'] - X['Opened'])]\n",
    "X['Days'] = pd.Series(days_processed)\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove Changed and Opened\n",
    "X.drop([\"Changed\", \"Opened\"], inplace=True, axis=1)\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text - summary\n",
    "\n",
    "One of the challenging types of features to analyze is a textual features. It does not make sense to convert longer text to one hot encoding since it is very unlikely that exactly the same text appear twice. \n",
    "\n",
    "The simplest method to extract features from a text is so-called bag of words. First, a vocabulary is created and each word in a vocabulary consistutes a features on its own. Usually, we limit the number of feature and exclude \"stop words\" (words/tokens that appear very often without any special meaning). In some cases it is also good to include not only single words as features but also pairs, triples - called n-grams.\n",
    "\n",
    "Let's create a simple bag of words for Summary using the CountVectorizer class (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's first make sure that we don't have any NaN values as summaries.\n",
    "X['Summary'] = X['Summary'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create CountVectorizer class; we take only 5000 most frequently appearing features\n",
    "count_vect = CountVectorizer(max_features=5000, stop_words=\"english\")\n",
    "\n",
    "# CountVectorizer fit method extracts vocabulary while transform performs the transformation. There is also\n",
    "# the method fit_transform that does both.\n",
    "bag_of_words = count_vect.fit_transform(list(X['Summary'])).todense()\n",
    "\n",
    "# We create a list of names of columns \n",
    "colnames = [\"Summary_\"+x for x in sorted(count_vect.vocabulary_.keys())]\n",
    "\n",
    "# Finally, we create a dataframe with bag of words features\n",
    "summary_bow = pd.DataFrame(bag_of_words, columns=colnames)\n",
    "summary_bow.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now merge the bag of words with X\n",
    "X = pd.concat([X.reset_index(drop=True), summary_bow], axis=1)\n",
    "X.drop([\"Summary\"], inplace=True, axis=1)\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier\n",
    "\n",
    "Let's train a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an instance of the classifier; a forest of 60 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, let's randomly split our data into a training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's train our random forest \n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can use the trained model to classify new instances\n",
    "y_pred = random_forest.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since we know what are the true classes, we can calculate different prediction quality measures, e.g., \n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average='macro')\n",
    "rec = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\"Accuracy = {:.3f}, Precision = {:.3f}, Recall = {:.3f}, F1-score = {:.3f}\".format(acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also analyze accuracy using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_confusion_matrix(cnf_matrix, classes=y_encoder.classes_,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=y_encoder.classes_, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we validated accuracy using test / train split. However, we very often use cross-validation for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_pred = cross_val_predict(random_forest, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y, y_pred)\n",
    "prec = precision_score(y, y_pred, average='macro')\n",
    "rec = recall_score(y, y_pred, average='macro')\n",
    "f1 = f1_score(y, y_pred, average='macro')\n",
    "\n",
    "\"Accuracy = {:.3f}, Precision = {:.3f}, Recall = {:.3f}, F1-score = {:.3f}\".format(acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_confusion_matrix(cnf_matrix, classes=y_encoder.classes_,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=y_encoder.classes_, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1. Look into the documentation of CountVectorizer class \n",
    "(http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) \n",
    "and change code creating bag of words so it:\n",
    "- takes into account bi-grams\n",
    "- is 0/1 feature (a word is present in the text or not; and not its frequency in the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2. There is also a column 'Assignee' that we didn't use. Create two features:\n",
    "- Eclipse Assignee - 1 if 'Assignee' ends with 'eclipse' (str.endswith('eclipse'))\n",
    "- Inbox Assignee - 1 if 'Assignee' ends with '-inbox' (str.endswith('-inbox'))\n",
    "\n",
    "Add the new feature and see if it improved the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task3. Transform the 'Text' column to a bag of words form. Experiment with different n-grams (unigrams, bigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Ann has a dog.\", \"Dog likes to eat.\", \"Ann likes to play with a dog.\"]\n",
    "texts_df = pd.DataFrame(pd.Series(texts, name=\"Text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
