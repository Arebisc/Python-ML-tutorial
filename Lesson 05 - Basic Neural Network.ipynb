{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 05 - Basic Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:46:46.673440Z",
     "start_time": "2019-12-17T20:46:46.528758Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, MinMaxScaler, RobustScaler\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling1D, Activation, Input, LSTM, GRU, Dense, Dropout, Flatten, Embedding, SpatialDropout1D, Bidirectional, CuDNNGRU\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Flatten, concatenate\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "from keras import backend as K\n",
    "\n",
    "# This part required only for GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "seed = 10293239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:41:04.034733Z",
     "start_time": "2019-12-17T20:41:03.887050Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and basic info\n",
    "\n",
    "Let's load the same dataset as in Lesson 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:12:02.688536Z",
     "start_time": "2019-12-17T20:12:02.243028Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bugs = pd.read_csv('./data/bugs_train.csv', parse_dates=['Opened', 'Changed'], index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:12:03.168014Z",
     "start_time": "2019-12-17T20:12:03.080725Z"
    }
   },
   "outputs": [],
   "source": [
    "bugs.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The classification task (the problem to solve)\n",
    "\n",
    "Our task remains the same for this lesson - we would be to predict what will be the resolution of the defect report (y) based on the description of a defect (X). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation (features)\n",
    "\n",
    "Let's quickly replicate processing of the Component and Severity features, as well as converting the decision class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:12:17.273517Z",
     "start_time": "2019-12-17T20:12:04.181632Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will make a copy of the main data\n",
    "bugs_small = bugs[[\"Assignee\", \"Component\", \"Severity\", \"Status\", \"Priority\", \"Opened\", \"Changed\", \"Summary\", \"Resolution\"]]\n",
    "\n",
    "# Component\n",
    "bugs_small = pd.get_dummies(bugs_small, columns=['Component'], prefix=\"Component\")\n",
    "\n",
    "# Severity\n",
    "bugs_small['Severity'] = bugs_small['Severity'].map(\n",
    "    {'enhancement':0, 'trivial':1, 'minor':2, 'normal':3, 'major':4, 'critical':5, 'blocker':6})\n",
    "\n",
    "# Status\n",
    "bugs_small['Status'] = bugs_small['Status'].map(\n",
    "    {'VERIFIED':0, 'RESOLVED':1, 'CLOSED':2})\n",
    "\n",
    "# Priority\n",
    "bugs_small['Priority'] = bugs_small['Priority'].map(\n",
    "    {'P1':1, 'P2':2, 'P3':3, 'P4':4, 'P5':5})\n",
    "\n",
    "\n",
    "y = bugs_small['Resolution']\n",
    "X = bugs_small.drop(['Resolution'], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "# Days\n",
    "X['Days'] = X.apply(lambda x: (x.Changed - x.Opened).days, axis=1)\n",
    "X.drop([\"Changed\", \"Opened\"], inplace=True, axis=1)\n",
    "\n",
    "# Summary as BoW\n",
    "X['Summary'] = X['Summary'].fillna('')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create CountVectorizer class; we take only 30 most frequently appearing features\n",
    "count_vect = CountVectorizer(max_features=30, stop_words=\"english\")\n",
    "\n",
    "# CountVectorizer fit method extracts vocabulary while transform performs the transformation. There is also\n",
    "# the method fit_transform that does both.\n",
    "bag_of_words = count_vect.fit_transform(list(X['Summary'])).todense()\n",
    "\n",
    "# We create a list of names of columns \n",
    "colnames = [\"Summary_\"+x for x in sorted(count_vect.vocabulary_.keys())]\n",
    "\n",
    "# Finally, we create a dataframe with bag of words features\n",
    "summary_bow = pd.DataFrame(bag_of_words, columns=colnames)\n",
    "X = pd.concat([X.reset_index(drop=True), summary_bow], axis=1)\n",
    "X.drop([\"Summary\"], inplace=True, axis=1)\n",
    "\n",
    "# Assignee\n",
    "inbox = [1 if x.endswith('-inbox') else 0 for x in bugs_small['Assignee']]\n",
    "X.insert(loc=0, column='Assignee_Inbox', value=pd.Series(inbox))\n",
    "    \n",
    "eclipse = [1 if x.endswith('eclipse') else 0 for x in bugs_small['Assignee']]\n",
    "X.insert(loc=0, column='Assignee_Eclipse', value=pd.Series(eclipse))\n",
    "X.drop([\"Assignee\"], inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:18:20.242434Z",
     "start_time": "2019-12-17T20:18:20.200274Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify the problem to binary classification => resulolution FIXED, NOT FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:20:24.737482Z",
     "start_time": "2019-12-17T20:20:24.712918Z"
    }
   },
   "outputs": [],
   "source": [
    "y_binary = np.array([1 if x == \"FIXED\" else 0 for x in y])\n",
    "y_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single layer multiple perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:44:25.704136Z",
     "start_time": "2019-12-17T20:44:25.597042Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(n_features=None):\n",
    "    \n",
    "    global seed\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    main_input = Input(shape=(n_features,), name='input')\n",
    "    output = Dense(1, activation=\"sigmoid\")(main_input)\n",
    "    \n",
    "    model = Model([main_input] , output)\n",
    "    algorithm = Adam(lr=0.005, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=algorithm, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model_1l(n_features=None):\n",
    "    \n",
    "    global seed\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    main_input = Input(shape=(n_features,), name='input')\n",
    "    model = Dense(10, activation=\"relu\")(main_input)\n",
    "    model = Dropout(0.2)(model)  \n",
    "    output = Dense(1, activation=\"sigmoid\")(model)\n",
    "    \n",
    "    model = Model([main_input] , output)\n",
    "    algorithm = Adam(lr=0.005, beta_1=0.95, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=algorithm, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:47:23.591287Z",
     "start_time": "2019-12-17T20:47:23.568617Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "set_random_seed(seed)\n",
    "\n",
    "callbacks_list = [\n",
    "    ReduceLROnPlateau( \n",
    "        monitor='loss',\n",
    "        min_lr=0.001, \n",
    "        factor=0.5,\n",
    "        verbose=1,\n",
    "        patience=10) \n",
    "]\n",
    "\n",
    "pipeline = []\n",
    "pipeline.append(('minmax', MinMaxScaler()))\n",
    "pipeline.append(('classifier', KerasClassifier(build_fn=get_model,  \n",
    "                                        epochs=20,\n",
    "                                        batch_size=128, \n",
    "                                        verbose=2, \n",
    "                                        callbacks=callbacks_list,\n",
    "                                        n_features=X.shape[1]))) \n",
    "\n",
    "model = Pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some simple evaluation using test-train split strategy (it will be much faster than waiting for CV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:47:25.094299Z",
     "start_time": "2019-12-17T20:47:25.014678Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_binary, test_size=0.33, random_state=seed, stratify=y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:47:37.416914Z",
     "start_time": "2019-12-17T20:47:26.140539Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:40:32.269203Z",
     "start_time": "2019-12-17T20:40:32.126801Z"
    },
    "collapsed": true
   },
   "source": [
    "Let's predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:47:48.313917Z",
     "start_time": "2019-12-17T20:47:48.062254Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:47:49.215363Z",
     "start_time": "2019-12-17T20:47:49.134757Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average='macro')\n",
    "rec = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\"Accuracy = {:.3f}, Precision = {:.3f}, Recall = {:.3f}, F1-score = {:.3f}\".format(acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T20:47:52.282508Z",
     "start_time": "2019-12-17T20:47:51.509772Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_confusion_matrix(cnf_matrix, classes=(\"NOT FIXED\", \"FIXED\"),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=(\"NOT FIXED\", \"FIXED\"), normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
